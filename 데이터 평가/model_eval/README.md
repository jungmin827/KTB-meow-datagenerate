# 성능 평가

---
## 성능 테스트 유형

> **동물 유형 (post_type)**
  - cat (고양이)
  - dog (강아지)

> **감정 (emotion)**
  - normal (일반)
  - happy (기쁨)
  - sad (슬픔)
  - angry (분노)
  - grumpy (까칠)
  - curious (호기심)

---

## 실행 방법

> **의존성 설치**
   ```bash
   export PYTHONPATH=$PYTHONPATH:/Users/seo/Documents/_code/for_AI/my_project/Finetuning/model_eval/KoBERTScore
   export PYTHONPATH=$PYTHONPATH:/Users/jaeseoksee/Documents/project/for_AI/my_project/Finetuning/model_eval/KoBERTScore

   pip install -r requirements.txt
   ```

> **평가 실행**
   ```bash
  streamlit run app.py
   ```

> **결과 확인**
   - `_output/` 폴더에 평가 결과(jsonl) 생성
   - 평균 점수, 항목별 통계 등 확인 가능

---


# 말투 변환/자연어 생성 모델 평가 지표 통합 정리

## 1. BLEU Score

| 평가 항목      | 평가 방식                                            | 점수 해석/의미         |
| ---------- | ------------------------------------------------ | ---------------- |
| BLEU (raw) | reference(원문)와 hypothesis(생성문)의 단어 단위 n-gram 중첩률 | 0\~1.0, 높을수록 유사함 |
| BLEU (정규화) | BLEU 점수×2, 최대 1.0으로 클리핑                          | 점수 분포 확대         |

**정의/설명:**
BLEU(Bilingual Evaluation Understudy)는 기계번역, 문장 변환 등에서 "원문(reference)과 생성문(hypothesis)의 표면적 유사도"를 정량화하는 대표적 자동화 지표입니다.

* n-gram(1\~4) 기준으로 단어 배열이 얼마나 일치하는지 평가
* Smoothing을 적용해 짧은 문장도 적절히 점수화
* **높을수록 두 문장이 어휘·어순 면에서 유사함을 의미**

---

## 2. KoBERTScore

| 평가 항목          | 평가 방식                                   | 점수 해석/의미             |
| -------------- | --------------------------------------- | -------------------- |
| KoBERTScore F1 | 사전학습 한국어 BERT 계열 모델(beomi/kcbert-base) 임베딩을 이용한 의미적 유사도 평가 | 0\~1.0, 높을수록 의미가 비슷함 |

**정의/설명:**
KoBERTScore는 BERT 기반 임베딩으로 reference와 candidate(생성문)의 \*\*"의미적 유사성"\*\*을 평가합니다.

* BLEU와 달리 문장 구조 변화, 어휘 다름에도 문맥/의미까지 비교
* 일반적으로 **0.8 이상이면 상당히 의미가 유사**
* 실제로 자연스러운 변환·재구성에도 높은 점수를 받을 수 있음

---

## 3. Perplexity Score

| 평가 항목            | 평가 방식                                 | 점수 해석/의미                    |
| ---------------- | ------------------------------------- | --------------------------- |
| Raw Perplexity   | 사전학습 언어모델(skt/kogpt2-base-v2)로 산출한 문장 예측 난이도(PPL) | 낮을수록(60\~180) 자연스러움         |
| Perplexity Score | PPL 구간별 0.2\~1.0 점수 부여 (코드상 5등급)      | 1.0: 매우 자연스러움, 0.2: 비문/품질저하 |

**정의/설명:**
Perplexity(혼란도, PPL)는 **언어모델이 해당 문장을 예측하기 얼마나 쉬운지**를 수치화하는 지표입니다.

* 낮을수록 더 자연스럽고 유창한 문장
* 코드상 자연스러운 구간(60\~180)만 1.0점 부여, 그 외 구간별 감점
* **비문, 반복 등 비정상 문장은 낮은 점수**

---

## 4. Quality Score

| 평가 항목                | 평가 방식                                       | 점수 해석/의미              |
| -------------------- | ------------------------------------------- | --------------------- |
| Forbidden Word Score | 금지어(욕설/비속어 등) 포함 여부 (포함: 0, 미포함: 1)         | 1.0: 무해, 0.0: 유해      |
| Repetition Score     | 단어/문장/이모지 반복 빈도 (3회 이하: 1, 초과시 감점)          | 반복 많을수록 감점            |
| Allowed Char Score   | 허용 문자(한글, 영문, 특수기호, 이모지) 비율                 | 비율 높을수록 자연스러운 표현      |
| Emoji Usage Score    | 이모지 적정(1\~3개) 사용: 1, 미사용: 0.5, 과다(4개 이상): 0 | 감정 표현이 너무 부족/과다 모두 감점 |
| Quality Score        | 위 4개 항목 평균(0\~1)                            | 품질 전반의 종합 점수          |

**정의/설명:**
Quality Score는 **욕설/비속어, 반복성, 허용 문자, 이모지 활용 등 실제 서비스에 필요한 품질 관리 기준**을 종합적으로 수치화한 지표입니다.

* 자연어 처리 파이프라인에서 **비정상, 유해, 반복성 등 실사용 이슈** 탐지
* 점수가 낮으면 실제로 서비스 사용에 부적합한 변환문 가능성이 큼

---

## 5. Type Score

| 평가 항목      | 평가 방식                              | 점수 해석/의미                         |
| ---------- | ---------------------------------- | -------------------------------- |
| Type Score | post\_type(고양이/강아지)와 일치하는 말투 패턴 탐지 | 1.0: 해당 동물 말투 완벽 반영              |
|            |                                    | 0.2: 두 말투 혼합, 0.1: 반대 동물만, 0: 없음 |

**정의/설명:**
Type Score는 **변환문이 목표 동물(post\_type)의 특성(어미, 의성어, 어감 등)을 실제로 잘 반영했는지** 판단합니다.

* 해당 동물 말투 패턴만 있으면 1.0점
* 반대 동물 패턴 있으면 0.1, 혼합 시 0.2, 둘 다 없으면 0
* **콘셉트형 말투 변환/감정형 챗봇 등 스타일 변환 성능 검증에 필수**

---

## 종합 활용 예시

* 각 평가 점수는 0.0\~1.0 범위(일부 raw 점수 제외)로, **높을수록 “더 자연스럽고, 의미가 비슷하며, 스타일에 맞는 변환”임을 의미**
* BLEU/KoBERTScore/Perplexity는 **문장 자체의 유사성·자연스러움**을, Quality/Type Score는 **실제 서비스 적합성·스타일 반영**을 강조
* 실제 평가시, 여러 지표를 종합해 **모델 비교/최적화/데이터 필터링**에 활용

---

## 주요 Scorer 의미 요약

| 평가 지표            | 의미와 쓰임(핵심)                                   |
| ---------------- | -------------------------------------------- |
| BLEU Score       | “표면상 단어 배열 유사도” — 번역·문장 변환 품질 정량 비교          |
| KoBERTScore      | “의미적 유사도” — 구조가 달라도 의미만 맞으면 높은 점수            |
| Perplexity Score | “문장 자연스러움” — 언어모델 관점, 낮을수록 더 자연스러운 문장        |
| Quality Score    | “서비스 품질 안전장치” — 금지어, 반복성, 이모지 활용 등 실사용 기준    |
| Type Score       | “스타일·콘셉트 반영” — 동물(고양이/강아지) 말투 등 특수한 말투 변환 평가 |

---

**위 평가 기준 및 지표 정의를 기반으로, 프로젝트별 커스텀 평가시 항목/스코어 해석/활용법을 쉽게 참고할 수 있습니다.**

* 필요한 경우, 각 평가 항목별 예시 문장 및 실제 분포 시각화도 추가 가능!
* 다른 지표/세부 항목을 추가하려면 동일 구조로 손쉽게 확장 가능합니다.

## 🧹 데이터 클랜징 스크립트 요약

- **대상**: jsonl 파일 내 `content`, `transformed_content` 필드 텍스트 정제
- **기능 요약**
  - 개행문자(`\r\n`, `\n`, `rn`) → 공백 처리
  - URL 자동 제거
  - 허용 문자(한글, 영문, 숫자, 주요 이모지, 구두점)만 남김
  - 동일 문자 4회 이상 반복 시 2회로 축소, 동일 이모지 4회 이상 반복 시 3회로 축소
  - 다중 공백 정리
- **사용법**
  - 입력 파일, 출력 파일 경로만 지정 후 실행
  - 지정 필드 값이 정제된 형태로 덮어써짐

**예시 실행**
```python
clean_jsonl_replace_fields("input.jsonl", "output.jsonl")
---